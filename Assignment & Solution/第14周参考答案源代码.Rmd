---
title: "第14周作业"
author: "2018201744 郭成城"
date: "12/17/2020"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: true
    lightbox: true
    toc_depth: 5
    gallery: false
    highlight: tango
    df_print: kable
    
---

<style>
#main .nav-pills > li.active > a,
#main .nav-pills > li.active > a:hover,
#main .nav-pills > li.active > a:focus {
background-color: #4a47a3;
}
#main .nav-pills > li > a:hover {
color: #FFFFFF !important;
background-color: #4a47a3;
}
#main .nav-pills > li.active > a,
#main .nav-pills > li.active > a:hover,
#main .nav-pills > li.active > a:focus {
color: #FFFFFF !important;
background-color: #4a47a3;
}

#main .nav-pills > li > a:hover {
background-color: #4a47a3;
}

h1, h2, h3, h4, h5, h6, legend {
color: #4a47a3;
}

#nav-top span.glyphicon {
color: #4a47a3;
}

#table-of-contents header {
color: #4a47a3;
}

#table-of-contents h2 {
background-color: #4a47a3;
}

#main a {
background-image: linear-gradient(180deg,#d64a70,#d64a70);
color: #c7254e;
}

a:hover {
color: #3d1308;
}

a:visited {
color: #3d1308;
}
#postamble .date {
font-size: 100%;
margin-bottom: 0px;
color: #dcd6f7;
}
#sidebar h2{
z-index:200;
background-color:#4a47a3;
text-align:center;
padding:0.809em;
display:block;
color:#fcfcfc;
font-size: 100%;
margin-top: 0px;
margin-bottom:0.809em;
}
</style>





```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### 新教材6.4 100名牙疾患者，先后经过两位不同的牙医的诊治，两位牙医在是否需要进行某项处理时给出的诊治方案不完全一致．现将两位牙医的不同意见数据列表如下，试分析两位医生的治疗方案是否完全一致．





<table>
<thead>
<tr class="header">
<th colspan="2"></th>
<th colspan="3"><center>牙医乙</center></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td  colspan="2"></td>
<td>需要处理</td>
<td>不需要处理</td>
<td>合计</td>
</tr>
<tr class="even">
<td rowspan="3">牙医甲</td>
<td>需要处理</td>
<td>40</td>
<td>5</td>
<td>45</td>
</tr>
<tr class="odd">
<td>不需要处理</td>
<td>25</td>
<td>30</td>
<td>55</td>
</tr>
<tr class="even">
<td>合计</td>
<td>65</td>
<td>35</td>
<td>100</td>
</tr>
</tbody>
</table>

对同一组研究对象进行两次独立评判，考察其一致性时，可以选择使用Kappa一致性检验，$$H_0:两位医生的治疗方案不一致 \quad v.s.\ \ H_1:两位医生的治疗方案一致$$
Kappa一致性检验下，定义$$P_0=\sum\limits_{i=1}^rp_{ii}$$$$P_e=\sum\limits_{i=1}^rp_i.p._i$$$P_e$表示的是一致性期望频率，那么$P_0-P_e$表示的是实际与独立判断结果概率之差，那么可以构建Kappa统计量$$K=\dfrac{P_0-P_e}{1-P_e}$$，它的方差为$$varK=\frac1{n(1-P_e)^2}[P_e+P_e^2-\sum p_i.p._i(p_i.+p._i)]$$
因此可以构造渐进正态的检验统计量$Z=\frac K{\sqrt{varK}}$。本题中，计算得$P_0=0.7,\ P_e=0.485$，Kappa统计量为$K=0.417,\ Z=4.53>1.96$，检验统计量值很大，远大于临界值，$p$值很小，因此有充足的理由拒绝$H_0$，认为两个牙医存在一致性。

```{r,warning=F,message=F}
#对角线概率和
P0=(40+30)/100

#一致性期望概率
Pe=.65*.45+.35*.55

K=(P0-Pe)/(1-Pe)
varK=1/100/(1-Pe)^2*(Pe+Pe^2-.65*.45*(.65+.45)-.35*.55*(.35+.55))
Z=K/sqrt(varK)

#检验p值
2*(1-pnorm(Z))
```

载入`irr`包，这个包提供了进行Kappa检验的多个函数，这里调用`kappa2(ratings, weight = c("unweighted", "equal", "squared"), sort.levels = FALSE)`，它可以对两个评分者对评分对一致性进行检验，这里在输入时把题目给定对列联表转换为了评分向量，向量中1值代表「需要处理」，0值代表「不需要处理」。最终得到了相同对$Z$值和检验结果。
```{R,message=F}
library(irr)
dat=data.frame(甲=c(rep(1,45),rep(0,55)),乙=c(rep(1,40),rep(0,5),rep(1,25),rep(0,30)))
kappa2(dat)
```


对于其中的`weight = c("unweighted", "equal", "squared")`参数，如果在有多个检查项目中有一个是为0的时候需要加权检验，而本题中只有「需要处理」与「不需要处理」两种取值，因此不需要进行加权，直接使用默认参数`unweighted`。

此外，`irr`中`kappam.fleiss`提供了对于多个评分者对数据进行一致性检验对方法。Fleiss' kappa系数适用于分析重复测量3次及以上且测量结果是无序分类变量的重测一致性或观察者一致性检验。取 irr 包中的diagnoses 数据集的一部分，截取前三个医生对 30 位病人的诊断结果，利用irr包中的`kappam.fleiss`函数实现Fleiss' kappa系数的计算。

得到的kappa系数值为0.534，p值接近0，认为数据间存在一致性。

```{r,warning=F,message=F}
data(diagnoses)
kappam.fleiss(diagnoses[,1:3])


```

### 新教材6.8 有关分位回归，回答以下问题．
#### （1）简述分位回归模型．

分位数回归是估计一组回归变量$X$与被解释变量$Y$的分位数之间线性关系的建模方法，由科恩克（Koenker）和巴塞特（Bassett）于1978年提出的，其基本思想是建立因变量$Y$对自变量$X$的条件分位数回归拟合模型：
$$Q_Y(\tau|X)=f(X)$$
其中，$\tau$是因变量$Y$在入条件下的分位数，$f(X)$拟合$Y$的第$\tau$分位数

传统的回归建立在被解释变量的条件期望的基础上，当$\varepsilon$满足正态和齐性（方差相等）条件时，可以用最小二乘法建立回归、预测模型，实际情况下，这两个假设往往得不到满足，比如左偏或右偏，用最小二乘拟合回归模型稳定性很差，分位回归对分位数进行回归，不需要分布和齐性方面过强的假设，在非正态和非齐性的情况下也能较好地把握数据的主要规律，分位回归以其稳健的性质己经开始在经济和医学领域广泛应用。

#### （2）简述分位回归模型参数估计的最优化问题．

线性分位回归可以通过极小化残差绝对值加权求和，即在绝对值前增加分位点权重系数：$$\hat\beta=\mathop{argmin}\limits_{\beta\in\mathbb{r,warning=F,message=F}^p}\sum_{i=1}^n\rho_\tau(y_i-x_i^T\beta)$$，其中$$\tau(u)=\left\{
\begin{aligned}
&\tau u&,u\geq0\\
&(1-\tau)|u|&, u<0
\end{aligned}
\right.$$
因此等价为$$\hat\beta(\tau)=\mathop{argmin}\left[\sum_{i\in\{i:y_i\geq x_i^T\beta(\tau)\}}\tau|y_i-x_i^T\beta(\tau)|_++\sum_{i\in\{i:y_i< x_i^T\beta(\tau)\}}(1-\tau)|y_i-x_i^T\beta(\tau)|_-\right]$$
Koenker和奥利（Orey）（1993）运用运筹学中的单纯形法求解线性分位回归，其思想是：任选一个顶点，沿着可行解围成的多边形边界搜索，直到找到最优点。该算法估计出来的参数具有很好的稳定性，但是在处理大型数据时运算的速度会显著降低。

目前流行的还有内点算法（interior point method），和平滑算法（smoothing method)等。内点算法对于那些具有大量观察值和少量变量的数据集运算效率很高；平滑算法在理论上比较简单，它适合处理具有大量观察值以及很多变量的数据集。由于分位回归需要借助大量计算，模型的参数估计要比传统的线性回归模型的求解复杂。


#### （3）分位回归相比于线性回归的优点有哪些？为什么具备这些优点？


与线性最小二乘回归相比较，分位回归的优点体现在以下几方面：

1. 分位回归对模型中的随机误差项不需对分布做具体的假定，有广泛的适用性；
2. 分位回归没有使用连接函数描述因变量与自变量的相互关系，因此分位回归体现了数据驱动的建模思想；
3. 中位数回归的估计方法与最小二乘法相比，估计结果对离群值则表现的更加稳健，而且，分位数回归对误差项并不要求很强的假设条件，因此对于非正态分布而言，分位数回归系数估计量则更加稳健。；
4. 由分位回归解出的系列回归模型可更为全面地体现分布特点。


#### （4）用分位回归方法拟合光盘中的infant-birthweight数据，并进行解释．

##### 1> 数据探索性分析

infant-birthweight数据包括50000个观测值，和17个变量，在回归之前，观察数据结构后首先对数据进行如下对预处理：

```{r,warning=F,message=F}
library(quantreg)
dat<-read.csv("infant-birthweight.txt",sep=" ")

#birmon全取6，可以删除
all(dat$birmon==6)

#发现数据中tri.none变量全为0，若加入模型会导致设计矩阵非奇异，应该删除
sum(dat$tri.none)

#tri1+tri2+tri3+novisit加起来恒等于1，存在完全共线性，考虑删除novisit
all((dat$tri1+dat$tri2+dat$tri3+dat$novisit)==1)
```


1. id、ranid反应的是观测的编号，在回归中不起作用，考虑予以删除；
2. 数据中tri.none变量全为0，若加入模型会导致设计矩阵非奇异，予以删除；
3. birmon中的变量全部取6，同样会导致设计矩阵非奇异，予以删除；
4. tri1+tri2+tri3+novisit加起来恒等于1，存在完全共线性，选择删除其中一个，例如删除变量novisit

完成筛选后，可以对数据进行描述统计

|          |    weight  |      black    |   married    |    boy     |  
|---------:|------------|---------------|--------------|------------|
| Min      | 240        |  0.0000       |   0.0000     |  0.0000    |
|1st Qu    | 3062       |  0.0000       |  0.0000      |  0.0000    |
|Median    | 3402       |  0.0000       |  1.0000      |  1.0000    |  
| Mean     |  3371      | 0.1628        |  0.7126      |  0.5158    |
|3rd Qu    |  3720      |  0.0000       |  1.0000      |  1.0000    |
|Max       |  6350      | 1.0000        |  1.0000      |  1.0000    |
|          |  **tri1**  |   **tri2**    | **tri3**     | **ed.hs**  |      
| Min      | 0.0000     |    0.0000     | 0.00000      | 0.000      |
|1st Qu    | 1.0000     |  0.0000       | 0.00000      | 0.000      |
|Median    | 1.0000     |   0.0000      | 0.00000      | 0.000      |
| Mean     |0.8429      |  0.1268       | 0.02228      | 0.349      |
|3rd Qu    |1.0000      | 0.0000        | 0.00000      | 1.000      |
|Max       |1.0000      | 1.0000        | 1.00000      | 1.000      |
|          |**ed.smcol**|**ed.col**     |**mom.age**   |  **smoke** |     
| Min      | 0.0000     | 0.000         | -9.0000      | 0.0000     |
|1st Qu    | 0.0000     | 0.000         | -4.0000      | 0.0000     |
|Median    | 0.0000     | 0.000         |  0.0000      | 0.0000     |
| Mean     |0.2426      | 0.249         | 0.4161       | 0.1307     |
|3rd Qu    |0.0000      | 0.000         | 5.0000       | 0.0000     |
|Max       |1.0000      | 1.000         | 18.0000      | 1.0000     |
|          |**cigsper** | **m.wtgain**  | **mom.age2**|**m.wtgain2**|     
| Min      | 0.000      | -30.0000      |   0.00       |   0.0      | 
|1st Qu    | 0.000      | -8.0000       |   4.00       |  16.0      |
|Median    | 0.000      |   0.0000      | 16.00        |  64.0      |
| Mean     | 1.477      |   0.7092      |  32.99       |  166.3     |
|3rd Qu    | 0.000      |   9.0000      | 49.00        |  196.0     |
|Max       | 60.000     |  68.0000      | 324.00       |  4624.0    |

##### 2> 初步建模

在完成预处理后，使用包`quantreg`对数据进行$\tau=0.1,\ 0.2,...,0.9$分位数回归，结果如下：

```{r,warning=F,message=F}
#去除id与ranid项，以及birmon, tri.nonr, novisit
dat<-dat[,-c(1,2,10,11,21)]

f<-rq(weight~.,data=dat,tau=seq(0.1,0.9,0.1))
summary(f)
```

首先分析模型显著性，在各个分位数回归模型中，显著性总体来说都较佳。在大部分的分位数下，分位数回归的系数，但个别分位数，存在个别变量不显著，如`m.wtgain2`在某些分位数下p值过高等。

此外，相比0.8分位数下模型所有系数全部显著的情况，对于0.9分位数下的模型，回归系数的显著性下降较多，可能的原因是外侧的分位数回归极易受极端值的影响，导致回归不如分位数取值适中的模型稳健。

##### 3> 赤池信息量准则（AIC）


赤池信息量准则建立在熵的概念基础上，可以权衡所估计模型的复杂度和此模型拟合数据的优良性，其中$L$是似然函数。

$$AIC=-2ln(L)+2k$$

对于这九个分位数，计算各自的AIC值，AIC值越小的模型拟合越佳。可以看到结果与我们的预期相符，对于0.5分位数的AIC值最好最稳健，对于分位数取较大或较小的模型AIC值较高，拟合略差。

```{r,warning=F,message=F}
aic=AIC(f)
plot(seq(0.1,0.9,0.1),aic,xlab="τ",ylab="AIC")
```

因此以0.5分位数之下的模型为例，分析各变量对于被解释变量的影响：


$$\hat y_{(0.5)}=3217.33917-175.02032black+54.94456married +121.63809boy +107.94182tri1+105.30297tri2+103.34455tri3+15.95897ed.hs+29.45544ed.smcol+37.86871ed.col+7.09684 mom.age-0.35874mom.age^2-158.11346smoke-3.70373cigsper+7.03974m.wtgain+0.02142m.wtgain^2$$

```{r,messgae=F,warning=F}
f0.5=rq(weight~.,data=dat,tau=0.5)
summary(f0.5)
```


模型的显著性十分良好，模型所有变量的系数均在5%之下显著，接下来考察此模型的拟合优度

##### 3> 拟合优度（Goodness-of-Fit）

对于模型的拟合优度，Koenker 与 Machado提出了衡量拟合优度的指标$𝑅^1$，类似于OLS的拟合优度$R^2$，值越接近1则拟合越佳。

定义$V(\tau)=\min_b \sum\rho_\tau(y_i-x^T_ib)$，定义$\hat\beta(\tau)$与$\tilde\beta(\tau)$分别代表全模型与约束模型（通常是只包含截距的模型）的分位数回归系数的估计，$\hat V$与$\tilde V$分别代表全模型与约束模型的$V$值，Koenker 与 Machado定义的拟合优度为$$R^1=1-\frac {\hat V}{ \tilde V}$$

可见模型的$R^1$很高，达到了0.88以上，表明模型拟合很好。

```{r,warning=F,message=F}
rho <- function(u,tau=.5)u*(tau - (u < 0))
V <- function(resid,tau){sum(rho(resid, tau))}
R<- function(resid,tau){
  f0=rq(weight~0,data=dat,tau=tau)
  1-V(resid,tau)/V(f0$resid,f0$tau)}

R(f0.5$resid,0.5)
```

参考文献：

[1] Koenker, R and Machado, J (1999),


##### 4> 模型解释

|  (Intercept)   |      black      |   married   |      boy    |      tri1  |    tri2    |
|----------------|-----------------|-------------|-------------|------------|------------|
|  3217.339***   |  -175.020***    | 54.94***    |  121.638*** |  107.942***|  105.303** |
| **tri3**       |     **ed.hs**   | **ed.smcol**|**ed.col**   |**mom.age** | **smoke**  |
|  103.345**     | 15.959*         |  29.455**   |   37.869*** |  7.097***  | -158.113   |
|    **cigsper **|   **m.wtgain**  |**mom.age2** |**m.wtgain2**|            |            |
| -3.704**       |  7.040**        |-0.359**     | 0.0214*     |            |            | 

分析模型的系数，由于系数全部显著，那么对于新生儿体重的中位数来说，黑人、母亲吸烟行为、每天吸烟根数`cigsper`的增加都是导致其下降的重要因素。例如：

* 控制其他条件，黑人母亲的婴儿平均比白人母亲的婴儿要轻175克；
* 控制其他条件，吸烟母亲的婴儿平均比不吸烟母亲的婴儿要轻158克；
* 控制其他条件，母亲每天吸烟增加一支，新生儿体重平均下降3.7克。

除平方项形式的`mom.age`与`m.wtgain`外，其他因素的系数都显著为正，印证了这些变量对于新生儿体重的正向影响：

* 已婚母亲的婴儿平均比白人母亲的婴儿要多55克
* 男婴比女婴重122克；

对于两个平方项，

* 首先看`mom.age`，一阶系数为正，二阶系数为负，表明婴儿体重随妈妈的年龄增加先上升后下降
* `m.wtgain`一阶与二阶项均为正，可见孕期母亲体重增加越多，婴儿体重也越大

分析`tril`系列变量，

* 它表明从初期妊娠，到中期妊娠再到晚期妊娠，婴儿的体重是逐渐下降的

最后是教育系列变量

* 说明目前的学历从低于高中，到高中学历，到some college，再到college毕业，婴儿的体重依次显著增加。




### 新教材6.9 模拟实验分析：$(X,Z)$的真实关系满足$z=2(exp(—30·(x-0.25)^2)+sin(\pi x^2))$。从均匀分布$U(0，1)$中抽取100个$X$值，将这些数值从小到大排序，依次产生带有$N(0，1)$噪声的$Y$值，即：$y=z+N(0,1)$。这样的实验重复20次，得到$(X,Y)$观测值矩阵和真值矩阵$(X,Z)$,完成以下分析任务：

#### (1)绘制$(X,Y)$的散点图，并在散点图上添加由$(X,Z)$生成的真实函数曲线；

```{r,warning=F,message=F}
#均匀分布$U(0，1)$中抽取100个$X$值
n=100
set.seed(3434384)
X=runif(n)

#将这些数值从小到大排序
X=sort(X)

#依次产生带有$N(0，1)$噪声的$Y$值，即：$y=z+N(0,1)$
Z=2*(exp(-30*(X-0.25^2))+sin(pi*X^2))
Y=Z+rnorm(n)

#绘制$(X,Y)$的散点图
plot(X,Y)

#并在散点图上添加由$(X,Z)$生成的真实函数曲线
XX=seq(0,1,.01)
ZZ=2*(exp(-30*(XX-0.25^2))+sin(pi*XX^2))
lines(XX,ZZ,lty=2)


```


#### （2）求解中位数线性回归，0.25分位数线性回归和0.75分位数线性回归，和不带噪声的真实值进行比较，估计拟合的均方误差；

首先进行中位数线性回归，利用前题提到过的`rq`函数直接求得了中位数线性回归表达式：$$\hat Y_{med}(x)=1.18412+0.27329x$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
注意，进行比较的是不带噪声的原始数据$Z$，而不是带噪声的$Y$。可以计算得到中位数线性回归拟合的均方误差$$MSE_{med}=3.372061$$
```{r,warning=F,message=F}
#中位数线性回归
fmed=rq(Y~X,tau=0.5)
summary(fmed)
mean((predict(fmed)-Z)^2)
```

其次进行0.25分位数线性回归，利用前题提到过的`rq`函数求得回归表达式：$$\hat Y_{low}(x)=0.32309+0.39764x$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
可以计算得到0.25分位数线性回归拟合的均方误差$$MSE_{low}=4.527782$$高于中位数回归。

```{r,warning=F,message=F}
#0.25分位数线性回归
flow=rq(Y~X,tau=0.25)
summary(flow)
mean((predict(flow)-Z)^2)
```
最后进行0.75分位数线性回归：$$\hat Y_{low}(x)=2.79747-0.66313x$$

可以计算得到0.75位位数线性回归拟合的均方误差$$MSE_{low}=3.813042$$也不如中位数回归。
```{r,warning=F,message=F}

#0.75分位数线性回归
fhig=rq(Y~X,tau=0.75)
summary(fhig)
mean((predict(fhig)-Z)^2)
```

综上，从$MSE$角度来看，中位数回归拟合更好，$MSE$更低。下面通过可视化可以看到三种拟合都不是很理想，它们更偏向于$X$取0.2～1之间散点的信息，对于图像左上角的散点拟合很差。

```{r,warning=F,message=F}
par(family="PingFang SC")
plot(X,Y)
lines(XX,ZZ,lty=2)
abline(fmed,col="red")
abline(flow,col="blue")
abline(fhig,col="green")
legend("topright",c("中位数","0.25分位数","0.75分位数","真实曲线"),col=c("red","blue","green","black"),lty=c(1,1,1,2))
```

#### （3）将线性回归改为多项式为二阶表示型（模型中纳入$X^2$项）和四阶（模型中纳入$X^2$,$X^3$,$X^4$项），继续拟合数据，比较（2）和（3）拟合的结果有怎样的不同；

##### 1> 二阶表示型


进行中位数线性回归，利用前题提到过的`rq`函数直接求得了中位数线性回归表达式：$$\hat Y_{med}(x)=0.87906+2.21020x-2.31862x^2$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
可以计算得到中位数线性回归拟合的均方误差$$MSE_{med}=3.625185$$均方误差没有得到很明显的下降。
```{r,warning=F,message=F}
#中位数线性回归
X2=X^2
fmed=rq(Y~X+X2,tau=0.5)
summary(fmed)
mean((predict(fmed)-Z)^2)
```

其次进行0.25分位数线性回归，利用前题提到过的`rq`函数求得回归表达式：$$\hat Y_{low}(x)=-0.34496+3.85942x-3.47371x^2$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
可以计算得到0.25位数线性回归拟合的均方误差$$MSE_{low}=5.16927$$同样地，MSE没有得到降低。

```{r,warning=F,message=F}
#0.25分位数线性回归
flow=rq(Y~X+X2,tau=0.25)
summary(flow)
mean((predict(flow)-Z)^2)
```
最后进行0.75分位数线性回归：$$\hat Y_{low}(x)=4.28078-8.10778x+7.12867x^2$$

可以计算得到0.25分位数线性回归拟合的均方误差$$MSE_{high}=3.542667$$，在三种回归中最小，拟合的曲线最好。

```{r,warning=F,message=F}

#0.75分位数线性回归
fhig=rq(Y~X+X2,tau=0.75)
summary(fhig)
mean((predict(fhig)-Z)^2)
```

综上，从$MSE$角度来看，0.75分位数回归拟合更好，$MSE$更低，因为它兼顾了分位较高的点，包括下图中左上角的那些点。下面通过可视化可以看到，三种拟合仍然不是很理想，对于图像左上角的散点拟合仍然十分地差。

但相比（2）中使用直线拟合，对于图上左上角的散点有了一定的拟合趋势。

```{r,warning=F,message=F}
par(family="PingFang SC")
plot(X,Y)
lines(XX,ZZ,lty=2)
lines(XX,coef(fmed)[1]+coef(fmed)[2]*XX+coef(fmed)[3]*XX^2,col="red")
lines(XX,coef(flow)[1]+coef(flow)[2]*XX+coef(flow)[3]*XX^2,col="blue")
lines(XX,coef(fhig)[1]+coef(fhig)[2]*XX+coef(fhig)[3]*XX^2,col="green")
legend("topright",c("中位数","0.25分位数","0.75分位数","真实曲线"),col=c("red","blue","green","black"),lty=c(1,1,1,2))
```

##### 2> 四阶表示型


进行中位数线性回归，利用前题提到过的`rq`函数直接求得了中位数线性回归表达式：$$\hat Y_{med}(x)=6.06272-64.09093x+220.79924x^2-272.10382x^3+109.33261x^4$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
可以计算得到中位数线性回归拟合的均方误差$$MSE_{med}=0.9558599$$均方误差得到了很显著的下降。
```{r,warning=F,message=F}
#中位数线性回归
X3=X^3;X4=X^4
fmed=rq(Y~X+X2+X3+X4,tau=0.5)
summary(fmed)
mean((predict(fmed)-Z)^2)
```

其次进行0.25分位数线性回归，利用前题提到过的`rq`函数求得回归表达式：$$\hat Y_{low}(x)=2.88355-39.92793x+143.25446x^2-177.84004x^3+71.50042x^4$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
可以计算得到0.25分位数线性回归拟合的均方误差$$MSE_{low}=2.788887$$0.25分位数的MSE没有得到降低。

```{r,warning=F,message=F}
#0.25分位数线性回归
flow=rq(Y~X+X2+X3+X4,tau=0.25)
summary(flow)
mean((predict(flow)-Z)^2)
```
最后进行0.75分位数线性回归：$$\hat Y_{low}(x)=10.38074  -81.66619 x+239.31751 x^2 -263.70010x^3+95.95471x^4$$

可以计算得到0.25分位数线性回归拟合的均方误差$$MSE_{high}=2.287524$$，优于0.25分位数但不及中位数。

```{r,warning=F,message=F}

#0.75分位数线性回归
fhig=rq(Y~X+X2+X3+X4,tau=0.75)
summary(fhig)
mean((predict(fhig)-Z)^2)
```

综上，从$MSE$角度来看，中位数回归拟合更好，$MSE$达到最低，因为它兼顾了分位较高的点，包括下图中左上角的那些点。下面通过可视化可以看到，也印证中位数曲线拟合的结果。

**因此在本题的各种拟合方法中，四阶表示型的中位数回归效果最佳**。

```{r,warning=F,message=F}
par(family="PingFang SC")
plot(X,Y)
lines(XX,ZZ,lty=2)
lines(XX,coef(fmed)[1]+coef(fmed)[2]*XX+coef(fmed)[3]*XX^2+coef(fmed)[4]*XX^3+coef(fmed)[5]*XX^4,col="red")
lines(XX,coef(flow)[1]+coef(flow)[2]*XX+coef(flow)[3]*XX^2+coef(flow)[4]*XX^3+coef(flow)[5]*XX^4,col="blue")
lines(XX,coef(fhig)[1]+coef(fhig)[2]*XX+coef(fhig)[3]*XX^2+coef(fhig)[4]*XX^3+coef(fhig)[5]*XX^4,col="green")
legend("topright",c("中位数","0.25分位数","0.75分位数","真实曲线"),col=c("red","blue","green","black"),lty=c(1,1,1,2))
```


根据多项式的性质，我们可以猜想，使用更高阶的多项式进行中位数拟合，可以得到更佳的拟合结果。这里尝试五阶和六阶。可以看到，5阶明显比4阶拟合更好，但上升到6阶会存在过拟合。

```{r,warning=F,message=F}
#中位数线性回归
X5=X^5;X6=X^6
f6=rq(Y~X+X2+X3+X4+X5+X6,tau=0.5)
f5=rq(Y~X+X2+X3+X4+X5,tau=0.5)
list(mean((predict(f5)-Z)^2),mean((predict(f6)-Z)^2))

par(family="PingFang SC")
plot(X,Y)
lines(XX,ZZ,lty=2)
lines(XX,coef(f5)[1]+coef(f5)[2]*XX+coef(f5)[3]*XX^2+coef(f5)[4]*XX^3+coef(f5)[5]*XX^4+coef(f5)[6]*XX^5,col="red")
lines(XX,coef(f6)[1]+coef(f6)[2]*XX+coef(f6)[3]*XX^2+coef(f6)[4]*XX^3+coef(f6)[5]*XX^4+coef(f6)[6]*XX^5+coef(f6)[7]*XX^6,col="orange")
legend("topright",c("5阶中位数回归","6阶中位数回归","真实曲线"),col=c("red","orange","black"),lty=c(1,1,2))
```

#### （4）改变$Y$值的生成方式：$y=2(exp(—30(x-0.25)^2)+sin(\pi x^2))+N(0,(2x)^2)$内求解多项式为二阶（$X^2$）和四阶（$X^2$,$X^3$,$X^4$）的中位数、0.25分位数、0.75分位回归，将这些拟合线绘制到散点图上。比较（2）（3）（4）的数据分析，给出讨论。

这里重新生成数据，从图中不难看到，散点在真值周围的分布随着$X$的增大越来越离散。代码如下：

```{r,warning=F,message=F}
#均匀分布$U(0，1)$中抽取100个$X$值
n=100
set.seed(3434384)
X=runif(n)

#将这些数值从小到大排序
X=sort(X)

#依次产生带有$N(0，1)$噪声的$Y$值，即：$y=z+N(0,1)$
Z=2*(exp(-30*(X-0.25^2))+sin(pi*X^2))
Y=Z+rnorm(n,sd=2*X)

#绘制$(X,Y)$的散点图
plot(X,Y)

#并在散点图上添加由$(X,Z)$生成的真实函数曲线
XX=seq(0,1,.01)
ZZ=2*(exp(-30*(XX-0.25^2))+sin(pi*XX^2))
lines(XX,ZZ,lty=2)


```

##### 1> 一阶表示型


首先进行中位数线性回归，利用前题提到过的`rq`函数直接求得了中位数线性回归表达式：$$\hat Y_{med}(x)= 1.14941-0.03452x$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
可以计算得到中位数线性回归拟合的均方误差$$MSE_{med}=3.439844$$
```{r,warning=F,message=F}
#中位数线性回归
fmed=rq(Y~X,tau=0.5)
summary(fmed)
mean((predict(fmed)-Z)^2)
```

其次进行0.25分位数线性回归，利用前题提到过的`rq`函数求得回归表达式：$$\hat Y_{low}(x)=0.64108-0.37092 x$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
可以计算得到0.25分位数线性回归拟合的均方误差$$MSE_{low}=4.482389$$高于中位数回归。

```{r,warning=F,message=F}
#0.25分位数线性回归
flow=rq(Y~X,tau=0.25)
summary(flow)
mean((predict(flow)-Z)^2)
```
最后进行0.75分位数线性回归：$$\hat Y_{low}(x)=2.08818 + 0.55543  x$$

可以计算得到0.25分位数线性回归拟合的均方误差$$MSE_{high}=3.920418$$高于中位数回归，但与0.25分位数相当。
```{r,warning=F,message=F}

#0.75分位数线性回归
fhig=rq(Y~X,tau=0.75)
summary(fhig)
mean((predict(fhig)-Z)^2)
```

综上，从$MSE$角度来看，中位数回归拟合更好，$MSE$更低。下面通过可视化可以看到三种拟合都不是很理想，几乎都是水平线，对于图像左上角的散点信息没有任何偏向。

```{r,warning=F,message=F}
par(family="PingFang SC")
plot(X,Y)
lines(XX,ZZ,lty=2)
abline(fmed,col="red")
abline(flow,col="blue")
abline(fhig,col="green")
legend("topright",c("中位数","0.25分位数","0.75分位数","真实曲线"),col=c("red","blue","green","black"),lty=c(1,1,1,2))
```

##### 2> 二阶表示型


进行中位数线性回归，利用前题提到过的`rq`函数直接求得了中位数线性回归表达式：$$\hat Y_{med}(x)=0.69175 + 2.12554 x -2.01556 x^2$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
可以计算得到中位数线性回归拟合的均方误差$$MSE_{med}=3.761927$$均方误差没有得到很明显的下降。
```{r,warning=F,message=F}
#中位数线性回归
X2=X^2
fmed=rq(Y~X+X2,tau=0.5)
summary(fmed)
mean((predict(fmed)-Z)^2)
```

其次进行0.25分位数线性回归，利用前题提到过的`rq`函数求得回归表达式：$$\hat Y_{low}(x)= 0.22807  + 1.93747 x -2.45845 x^2$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
可以计算得到0.25分位数线性回归拟合的均方误差$$MSE_{low}=4.920684$$同样地，MSE没有得到降低。

```{r,warning=F,message=F}
#0.25分位数线性回归
flow=rq(Y~X+X2,tau=0.25)
summary(flow)
mean((predict(flow)-Z)^2)
```
最后进行0.75分位数线性回归：$$\hat Y_{low}(x)=3.36895-7.22886x+ 8.04999 x^2$$

可以计算得到0.25分位数线性回归拟合的均方误差$$MSE_{high}=3.539638$$，在三种回归中最小，拟合的曲线最好。

```{r,warning=F,message=F}

#0.75分位数线性回归
fhig=rq(Y~X+X2,tau=0.75)
summary(fhig)
mean((predict(fhig)-Z)^2)
```

综上，从$MSE$角度来看，0.75分位数回归拟合更好，$MSE$更低，因为它兼顾了分位较高的点，包括下图中左上角的那些点。下面通过可视化可以看到，三种拟合仍然不是很理想，对于图像左上角的散点拟合仍然十分地差。

```{r,warning=F,message=F}
par(family="PingFang SC")
plot(X,Y)
lines(XX,ZZ,lty=2)
lines(XX,coef(fmed)[1]+coef(fmed)[2]*XX+coef(fmed)[3]*XX^2,col="red")
lines(XX,coef(flow)[1]+coef(flow)[2]*XX+coef(flow)[3]*XX^2,col="blue")
lines(XX,coef(fhig)[1]+coef(fhig)[2]*XX+coef(fhig)[3]*XX^2,col="green")
legend("topright",c("中位数","0.25分位数","0.75分位数","真实曲线"),col=c("red","blue","green","black"),lty=c(1,1,1,2))
```

##### 3> 四阶表示型


进行中位数线性回归，利用前题提到过的`rq`函数直接求得了中位数线性回归表达式：$$\hat Y_{med}(x)=5.58963-62.80620x+221.61071x^2 -276.47705x^3+111.71180x^4$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
可以计算得到中位数线性回归拟合的均方误差$$MSE_{med}=1.146304$$均方误差得到了很显著的下降。
```{r,warning=F,message=F}
#中位数线性回归
X3=X^3;X4=X^4
fmed=rq(Y~X+X2+X3+X4,tau=0.5)
summary(fmed)
mean((predict(fmed)-Z)^2)
```

其次进行0.25分位数线性回归，利用前题提到过的`rq`函数求得回归表达式：$$\hat Y_{low}(x)= 4.78531-56.43084x+202.12547x^2 -263.16996x^3+112.38748x^4$$
均方误差的公式为$$MSE=\sum_{i=1}^n(\hat Y(x)-Z(x))^2/n$$
可以计算得到0.25分位数线性回归拟合的均方误差$$MSE_{low}= 1.986502$$0.25分位数的MSE没有得到降低。

```{r,warning=F,message=F}
#0.25分位数线性回归
flow=rq(Y~X+X2+X3+X4,tau=0.25)
summary(flow)
mean((predict(flow)-Z)^2)
```
最后进行0.75分位数线性回归：$$\hat Y_{low}(x)=9.11817 -77.62363 x+236.28089 x^2 -261.78994x^3+    94.46553  x^4$$

可以计算得到0.25分位数线性回归拟合的均方误差$$MSE_{high}=1.822718$$，在三种回归中表现最差。

```{r,warning=F,message=F}

#0.75分位数线性回归
fhig=rq(Y~X+X2+X3+X4,tau=0.75)
summary(fhig)
mean((predict(fhig)-Z)^2)
```

综上，从$MSE$角度来看，中位数回归拟合更好，$MSE$达到最低，因为它兼顾了分位较高的点，包括下图中左上角的那些点。下面通过可视化可以看到，的确是中位数曲线拟合的最好。

```{r,warning=F,message=F}
par(family="PingFang SC")
plot(X,Y)
lines(XX,ZZ,lty=2)
lines(XX,coef(fmed)[1]+coef(fmed)[2]*XX+coef(fmed)[3]*XX^2+coef(fmed)[4]*XX^3+coef(fmed)[5]*XX^4,col="red")
lines(XX,coef(flow)[1]+coef(flow)[2]*XX+coef(flow)[3]*XX^2+coef(flow)[4]*XX^3+coef(flow)[5]*XX^4,col="blue")
lines(XX,coef(fhig)[1]+coef(fhig)[2]*XX+coef(fhig)[3]*XX^2+coef(fhig)[4]*XX^3+coef(fhig)[5]*XX^4,col="green")
legend("topright",c("中位数","0.25分位数","0.75分位数","真实曲线"),col=c("red","blue","green","black"),lty=c(1,1,1,2))
```




### 数据：R galaxies {MASS} 数据背景可参考论文 Postman, M., Huchra, J. P. and Geller, M. J. (1986) Probes of large-scale structures in the Corona Borealis region. Astronomical Journal 92, 1238–1247.

#### 请用这个数据对velocities的分布密度尝试不同个的带宽和不同的核函数$\hat p_h(x)$（至少4种，任意选）作核估计，并绘制出来，对比差异。

使用R语言默认确定的带宽$h=1002$，

分别使用三角核、Epanechnikov核、高斯核以及余弦核，绘图如下图所示。这里带宽选择使用了R语言自带的参数`nrd`，代表利用高斯核估计带宽，计算得到带宽大小为1180：$$\hat h_0\approx 1.06\hat \sigma n^{(-1/5)}$$

```{r,warning=F,message=F}
library(MASS)
data(galaxies)

#三角核
par(family="PingFang SC")
plot(density(galaxies,kernel="triangular",bw="nrd"),main="核密度估计，bw=nrd",col="red")

#Epanechnikov核
lines(density(galaxies,kernel="epanechnikov",bw="nrd"),col="blue")

#高斯核
lines(density(galaxies,kernel="gaussian",bw="nrd"),col="green")

#余弦核
lines(density(galaxies,kernel="cosine",bw="nrd"))
legend("topleft",c('三角核','Epanechnikov','高斯核','余弦核'),col = c("red",'blue','green',"black"),lty=1)
```

使用`ucv`即无偏交叉验证法确定带宽，求得$bw=626$。

```{r,warning=F,message=F}
#三角核
par(family="PingFang SC")
plot(density(galaxies,kernel="triangular",bw="ucv"),main="核密度估计，bw=ucv",col="red")

#Epanechnikov核
lines(density(galaxies,kernel="epanechnikov",bw="ucv"),col="blue")

#高斯核
lines(density(galaxies,kernel="gaussian",bw="ucv"),col="green")

#余弦核
lines(density(galaxies,kernel="cosine",bw="ucv"))
legend("topleft",c('三角核','Epanechnikov','高斯核','余弦核'),col = c("red",'blue','green',"black"),lty=1)
```


使用`SJ`表示Sheather & Jones (1991)法确定带宽，求得$bw=643$，与无偏交叉验证法结果基本一致。

```{r,warning=F,message=F}
#三角核
par(family="PingFang SC")
plot(density(galaxies,kernel="triangular",bw="SJ"),main="核密度估计，bw=SJ",col="red")

#Epanechnikov核
lines(density(galaxies,kernel="epanechnikov",bw="SJ"),col="blue")

#高斯核
lines(density(galaxies,kernel="gaussian",bw="SJ"),col="green")

#余弦核
lines(density(galaxies,kernel="cosine",bw="SJ"))
legend("topleft",c('三角核','Epanechnikov','高斯核','余弦核'),col = c("red",'blue','green',"black"),lty=1)
```

画出四种核函数的核密度估计的图像后，我们可以观察到这四种核函数的差异所在：

可以看到，四种核函数的核密度估计基本一致，但特别在拐点处存在差异，三角核（红色）的转折点更尖锐，高斯核以及余弦核密度估计则比较平滑；高斯核（绿色）在凸点处密度较大，在凹点处密度较小；Epanechnikov核在拐点处平滑，但在非拐点处较扭曲且不稳定。


#### 请选择一个合适的$h$进行如下实验：用课堂上讲过的方法对该数据的$p_h(x)$通过bootstrap方法近似给出由$\hat p_h(x)$构造的置信水平为$95\%$置信带。

使用SJ法确定的带宽$h=643$进行实验，核密度bootstrap方法近似给出由$\hat p_h(x)$构造的置信水平为$95%$置信带的步骤如下：

1. 从经验分布$\hat F_n$中重抽样$X_1^*,X_2^*,...X_n^*$，每个样本点的概率均为$\frac 1n$
2. 基于Bootstrap样本$X_1^*,X_2^*,...X_n^*$，抽样计算$\hat p^*_h$
3. 计算$R=sup_x\sqrt{nh}||\hat p_h-\hat p_h||_\inf$
4. 重复步骤1～3$B$次，得到$R_1,R_2,...,R_B$
5. 令$z_\alpha$是$\{R_j,j=1,...,B\}$的$\alpha$分位数$$\frac 1B \sum_{j=1}^B I(R_j>z_\alpha)\approx \alpha$$
6. 令$$l_n(x)=\hat p_h(x)-\frac{z_\alpha}{\sqrt{nh}},\quad u_n(x)=\hat p(x)+\frac{z_\alpha}{\sqrt{nh}}$$

```{r,warning=F,message=F}
#设定迭代次数
B=1000
n=length(galaxies)
set.seed(342423)

#对原样本核密度估计
h=bw.SJ(galaxies)
org=density(galaxies,kernel="gaussian",bw=h,from=min(galaxies),to=max(galaxies))

test=max(org$x)
  
#数组保存R
R<-rep(0,B)

#进行B次迭代
for(i in 1:B)
{
  #有返回重抽样
  sam<-sample(galaxies,n,replace = T)
  
  #基于Bootstrap样本进行核密度估计
  den=density(sam,kernel="gaussian",bw=h,from=min(galaxies),to=max(galaxies))
  
  #求R
  R[i]=sqrt(n*h)*max(abs(org$y-den$y))
}

#寻找R的alpha分位数
z=quantile(R,0.05)

#求置信限
l=org$y-z/sqrt(n*h)
u=org$y+z/sqrt(n*h)

l[l<0]=0
u[u>1]=1
plot(org$x,org$y,type="l",lty=1,col="red",xlab='',ylab="Density",ylim=c(0,0.00023))
lines(org$x,l,lty=2,col="red")
lines(org$x,u,lty=2,col="red")
```

#### 尝试降低偏差的点估计方法给出$p(x)$的近似估计$\hat p(.)$，取$x$在最小值和最大值均匀变化，比较$\hat p_h(x)$,$\hat p_{2h}(x)$和$\hat p(x)$三个点估计的差异。

首先基于降低偏差的点估计方法给出$p(x)$的近似估计$\hat p(x)$。
$$\hat p(x)=\hat p_h(x)-\hat b(x)=\frac 43(\hat p_h(x)-\frac 14\hat p_{2h}(x))$$

下面编写R代码进行估计，$h$与上一题使用相同的取值：
```{r,warning=F,message=F}
ph=density(galaxies,kernel="gaussian",bw=h,from=min(galaxies),to=max(galaxies))

p2h=density(galaxies,kernel="gaussian",bw=2*h,from=min(galaxies),to=max(galaxies))

#降低偏差的点估计
p_hat=4/3*(ph$y-p2h$y/4)

#绘制图像
par(family="PingFang SC")
plot(p2h$x,p_hat,col="red",xlab="",ylab="Density",type='l')
lines(ph$x,ph$y,col="blue")
lines(p2h$x,p2h$y,col="orange")
legend("topright",c("降低偏差的点估计","\\hat p_h(x)","\\hat p_{2h}(x)"),col=c("red","blue","orange"),lty=1)

```


红线为降低偏差的点估计结果，从图像可以看出降低偏差的点估计曲线效果更好。